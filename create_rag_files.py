#!/usr/bin/env python3
"""
RAG ì‹œìŠ¤í…œì˜ ëª¨ë“  ìˆ˜ì •ëœ íŒŒì¼ì„ ìƒì„±í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
ì‹¤í–‰: python create_rag_files.py
"""
import os
from pathlib import Path
import sys
import importlib.util
sys.path.append('./utils')

# tool_reportgen ë™ì  import
spec = importlib.util.spec_from_file_location("tool_reportgen", os.path.join(os.path.dirname(__file__), "utils", "tool_reportgen.py"))
if spec is not None and spec.loader is not None:
    tool_reportgen = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(tool_reportgen)
    report_generation = tool_reportgen.report_generation
else:
    raise ImportError("Cannot import tool_reportgen module.")

def create_file(filepath, content):
    """íŒŒì¼ ìƒì„± í•¨ìˆ˜"""
    path = Path(filepath)
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, 'w', encoding='utf-8') as f:
        f.write(content)
    print(f"âœ“ Created: {filepath}")

# í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„±
print("RAG ì‹œìŠ¤í…œ íŒŒì¼ ìƒì„± ì‹œì‘...\n")

# 1. graph_builder.py
graph_builder_content = '''from __future__ import annotations
import json
from typing import Dict, Any, List, Literal

from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, BaseMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langchain_core.utils.function_calling import convert_to_openai_function

from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langgraph.graph.message import add_messages

from utils.tool_search import search_documents
from utils.tool_answer import final_answer
from utils.tool_diagnose import tool_diagnose
from state_types import GraphState
import os
os.environ["OPENAI_API_KEY"] = ""

# Tool registry - ì´ë¦„ í†µì¼
search_documents.name = "search_documents"  # ëª…ì‹œì ìœ¼ë¡œ ì´ë¦„ ì„¤ì •
final_answer.name = "final_answer"  # ì´ë¦„ í†µì¼
tool_diagnose.name = "tool_diagnose"
report_generation.name = "report_generation"

TOOLS = [search_documents, final_answer, tool_diagnose, report_generation]
tool_node = ToolNode(TOOLS)  # ToolNode ì‚¬ìš©

# System prompt - Function Calling ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •
system_prompt = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ì‹í’ˆì˜ì•½í’ˆì•ˆì „ì²˜ì—ì„œ ì˜ì•½ì œí’ˆ ì¸í—ˆê°€ë¥¼ ì§€ì›í•˜ê¸° ìœ„í•´ ê°œë°œëœ ë§¤ìš° ê°•ë ¥í•œ ì¸í—ˆìŠ¤í„´íŠ¸ nxj_llmì…ë‹ˆë‹¤.

<role>
** ì¤‘ìš”: ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ í™œìš©í•´ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì„ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
** ì¤‘ìš”: ì¶”ê°€ ê²€ì¦ ì „ê¹Œì§€ ì´ ì •ë³´ëŠ” ì°¸ê³ ìš©ì…ë‹ˆë‹¤.
</role>

<available_tools>
ë‹¹ì‹ ì€ ë‹¤ìŒ ë„êµ¬ë“¤ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
1. search_documents: ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
   - ì…ë ¥: query (ë¬¸ìì—´)
   - ì¶œë ¥: documents, sources, pathsë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬

2. final_answer: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
   - ì…ë ¥: answer (ë¬¸ìì—´), sources (ë¦¬ìŠ¤íŠ¸), paths (ë¦¬ìŠ¤íŠ¸)
   - ì¶œë ¥: ìµœì¢… ë‹µë³€ ë”•ì…”ë„ˆë¦¬

3. tool_diagnose: ë¬¸ì œ ë°œìƒ ì‹œ ì§„ë‹¨í•©ë‹ˆë‹¤.
   - ì…ë ¥: last_input (ë¬¸ìì—´), context (ë”•ì…”ë„ˆë¦¬)

4. report_generation: ì‚¬ìš©ìê°€ ë³´ê³ ì„œ/ë¦¬í¬íŠ¸ í˜•íƒœì˜ ë‹µë³€ì„ ìš”êµ¬í•  ë•Œ ë³´ê³ ì„œ í”Œë˜ë„ˆ ë° ì„¹ì…˜ ìƒì„± í”„ë¡¬í”„íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
   - ì…ë ¥: topic (ë¬¸ìì—´), report_organization (ë¬¸ìì—´, ì˜µì…˜), context (ë¬¸ìì—´, ì˜µì…˜), feedback (ë¬¸ìì—´, ì˜µì…˜)
   - ì¶œë ¥: ë³´ê³ ì„œ í”Œë˜ë„ˆ í”„ë¡¬í”„íŠ¸(ë¬¸ìì—´)
</available_tools>

<instructions>
1. ì‚¬ìš©ìê°€ "ë³´ê³ ì„œ" ë˜ëŠ” "ë¦¬í¬íŠ¸" í˜•íƒœì˜ ë‹µë³€ì„ ìš”êµ¬í•˜ë©´ ë°˜ë“œì‹œ report_generation toolì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.
2. ê·¸ ì™¸ì—ëŠ” ê¸°ì¡´ Q&A í”Œë¡œìš°(search_documents â†’ final_answer)ë¡œ ë™ì‘í•˜ì„¸ìš”.
3. ë¬¸ì„œì— ì •ë³´ê°€ ì—†ìœ¼ë©´ ê·¸ ì‚¬ì‹¤ì„ ëª…ì‹œí•˜ì„¸ìš”.
</instructions>
"""

# LLM setup with function calling
llm = ChatOpenAI(
    model="gpt-4-turbo",
    temperature=0.2,
    max_tokens=2048,
).bind_functions(TOOLS)  # Function binding

# Simplified State
class State(GraphState):
    messages: List[BaseMessage] = []
    documents: List[str] = []
    sources: List[str] = []
    paths: List[str] = []
    answer: str = ""

# Agent node
def agent(state: State) -> Dict[str, Any]:
    """LLMì´ ë‹¤ìŒ í–‰ë™ì„ ê²°ì •í•˜ëŠ” ë…¸ë“œ"""
    messages = state.get("messages", [])
    
    # System prompt ì¶”ê°€ (ì²« ë²ˆì§¸ ë©”ì‹œì§€ì¸ ê²½ìš°)
    if not any(isinstance(m, SystemMessage) for m in messages):
        messages = [SystemMessage(content=system_prompt)] + messages
    
    # LLM í˜¸ì¶œ
    response = llm.invoke(messages)
    
    # ë©”ì‹œì§€ ì—…ë°ì´íŠ¸
    return {"messages": [response]}

# Router function
def should_continue(state: State) -> Literal["tools", "end"]:
    """ë‹¤ìŒ ë…¸ë“œë¥¼ ê²°ì •í•˜ëŠ” ë¼ìš°í„°"""
    messages = state.get("messages", [])
    last_message = messages[-1] if messages else None
    
    # Function callì´ ìˆìœ¼ë©´ toolsë¡œ, ì—†ìœ¼ë©´ endë¡œ
    if last_message and hasattr(last_message, "additional_kwargs"):
        if last_message.additional_kwargs.get("function_call"):
            return "tools"
    
    return "end"

# Result processor
def process_result(state: State) -> Dict[str, Any]:
    """ìµœì¢… ê²°ê³¼ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë…¸ë“œ"""
    messages = state.get("messages", [])
    
    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ì—ì„œ ë‹µë³€ ì¶”ì¶œ
    for message in reversed(messages):
        if isinstance(message, AIMessage) and message.content:
            # final_answer ë„êµ¬ì˜ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
            if "answer" in state and state["answer"]:
                return {
                    "messages": messages,
                    "answer": state["answer"]
                }
            # ì¼ë°˜ AI ë©”ì‹œì§€
            return {
                "messages": messages,
                "answer": message.content
            }
    
    return {
        "messages": messages,
        "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    }

# Tool result handler
def handle_tool_result(state: State) -> Dict[str, Any]:
    """ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ Stateì— ë°˜ì˜"""
    messages = state.get("messages", [])
    
    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ tool ì‘ë‹µì¸ì§€ í™•ì¸
    if messages and hasattr(messages[-1], "content"):
        try:
            # Tool ì‘ë‹µ íŒŒì‹±
            content = messages[-1].content
            if isinstance(content, str) and content.startswith("{"):
                result = json.loads(content)
                
                # search_documents ê²°ê³¼ ì²˜ë¦¬
                if "documents" in result:
                    return {
                        "documents": result.get("documents", []),
                        "sources": result.get("sources", []),
                        "paths": result.get("paths", []),
                        "messages": messages
                    }
                
                # final_answer ê²°ê³¼ ì²˜ë¦¬
                if "answer" in result:
                    return {
                        "answer": result.get("answer", ""),
                        "sources": result.get("sources", []),
                        "paths": result.get("paths", []),
                        "messages": messages
                    }
        except:
            pass
    
    return {"messages": messages}

# Build graph
workflow = StateGraph(State)

# Add nodes
workflow.add_node("agent", agent)
workflow.add_node("tools", tool_node)
workflow.add_node("process_result", process_result)
workflow.add_node("handle_tool_result", handle_tool_result)

# Set entry point
workflow.set_entry_point("agent")

# Add edges
workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "tools": "tools",
        "end": "process_result"
    }
)
workflow.add_edge("tools", "handle_tool_result")
workflow.add_edge("handle_tool_result", "agent")
workflow.add_edge("process_result", END)

# Compile
app = workflow.compile()

# Simple interface
def respond(state: dict) -> dict:
    """
    ê°„ë‹¨í•œ ì¸í„°í˜ì´ìŠ¤ í•¨ìˆ˜
    """
    # ì…ë ¥ ê²€ì¦
    messages = state.get("messages", [])
    query = state.get("query", "") or state.get("last_user", "")
    
    if not messages and query:
        messages = [HumanMessage(content=query)]
    
    if not messages:
        return {
            "messages": [],
            "output": "âŒ ì…ë ¥ ì˜¤ë¥˜: ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.",
            "answer": ""
        }
    
    # ì´ˆê¸° ìƒíƒœ ì„¤ì •
    initial_state = {
        "messages": messages,
        "documents": [],
        "sources": [],
        "paths": [],
        "answer": ""
    }
    
    try:
        # Graph ì‹¤í–‰
        result = app.invoke(initial_state)
        
        # ê²°ê³¼ ì¶”ì¶œ
        answer = result.get("answer", "")
        if not answer:
            # ë©”ì‹œì§€ì—ì„œ ë‹µë³€ ì¶”ì¶œ ì‹œë„
            for msg in reversed(result.get("messages", [])):
                if isinstance(msg, AIMessage) and msg.content:
                    answer = msg.content
                    break
        
        return {
            "messages": result.get("messages", []),
            "output": answer,
            "answer": answer,
            "sources": result.get("sources", []),
            "paths": result.get("paths", [])
        }
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        
        # ì˜¤ë¥˜ ì§„ë‹¨
        diagnosis = tool_diagnose._run(
            last_input=query,
            context={"error": str(e), "state": initial_state}
        )
        
        return {
            "messages": messages,
            "output": f"âŒ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {str(e)}\\n\\nğŸ” ì§„ë‹¨ ê²°ê³¼:\\n{diagnosis}",
            "answer": ""
        }
'''

# 2. state_types.py
state_types_content = '''from typing_extensions import TypedDict
from typing import List
from langchain_core.messages import BaseMessage

class GraphState(TypedDict, total=False):
    """LangGraph State ì •ì˜ - í•„ìˆ˜ í•„ë“œë§Œ ìœ ì§€"""
    # í•µì‹¬ í•„ë“œ
    messages: List[BaseMessage]  # LangGraph ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬
    
    # ê²€ìƒ‰ ê´€ë ¨ í•„ë“œ
    documents: List[str]  # ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©
    sources: List[str]    # ë¬¸ì„œ ì¶œì²˜ (íŒŒì¼ëª…)
    paths: List[str]      # ë¬¸ì„œ ê²½ë¡œ
    
    # ê²°ê³¼ í•„ë“œ
    answer: str          # ìµœì¢… ë‹µë³€
    
    # ì„ íƒì  í•„ë“œ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±)
    query: str           # ì‚¬ìš©ì ì§ˆë¬¸
    last_user: str       # ë§ˆì§€ë§‰ ì‚¬ìš©ì ì…ë ¥ (í˜¸í™˜ì„±)
'''

# 3. run_gradio.py
run_gradio_content = '''import gradio as gr
from graph_builder import respond as graph_respond
from langchain_core.messages import HumanMessage, AIMessage
import json
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="[%(asctime)s] %(levelname)s: %(message)s",
)

# ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬
conversation_history: list[dict] = []

def extract_answer(response: dict) -> str:
    """ì‘ë‹µì—ì„œ ë‹µë³€ ì¶”ì¶œ"""
    # 1. answer í•„ë“œ í™•ì¸
    if response.get("answer"):
        return response["answer"]
    
    # 2. output í•„ë“œ í™•ì¸
    if response.get("output"):
        return response["output"]
    
    # 3. messagesì—ì„œ ë§ˆì§€ë§‰ AI ë©”ì‹œì§€ ì¶”ì¶œ
    messages = response.get("messages", [])
    for msg in reversed(messages):
        if isinstance(msg, AIMessage) and msg.content:
            # JSON í˜•íƒœì˜ ì‘ë‹µì¸ì§€ í™•ì¸
            content = msg.content
            if isinstance(content, str) and content.strip().startswith("{"):
                try:
                    data = json.loads(content)
                    if "answer" in data:
                        return data["answer"]
                except:
                    pass
            return content
    
    return "âŒ ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

def respond_wrapper(message: str, chat_history: list | None = None):
    """Gradio ì¸í„°í˜ì´ìŠ¤ë¥¼ ìœ„í•œ ì‘ë‹µ í•¨ìˆ˜"""
    logging.info(f"USER: {message}")
    
    if not message.strip():
        return chat_history or []
    
    # íˆìŠ¤í† ë¦¬ ê´€ë¦¬
    history = chat_history or []
    user_query = message.strip()
    
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
    conversation_history.append({"role": "user", "content": user_query})
    
    # LangChain ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    messages = []
    for msg in conversation_history:
        if msg["role"] == "user":
            messages.append(HumanMessage(content=msg["content"]))
        elif msg["role"] == "assistant":
            messages.append(AIMessage(content=msg["content"]))
    
    # Graph ìƒíƒœ ìƒì„±
    state = {
        "messages": messages,
        "query": user_query,
        "last_user": user_query
    }
    
    try:
        # Graph ì‹¤í–‰
        response = graph_respond(state)
        logging.debug(f"Graph response: {response}")
        
        # ë‹µë³€ ì¶”ì¶œ
        answer = extract_answer(response)
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        conversation_history.append({"role": "assistant", "content": answer})
        
        # UIì— í‘œì‹œí•  íˆìŠ¤í† ë¦¬ (ìµœê·¼ ëŒ€í™”ë§Œ)
        visible_history = [
            {"role": "user", "content": user_query},
            {"role": "assistant", "content": answer}
        ]
        
        logging.info(f"ASSISTANT: {answer[:100]}...")
        return visible_history
        
    except Exception as e:
        logging.exception("Error in graph_respond")
        error_msg = f"âš ï¸ ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        
        visible_history = [
            {"role": "user", "content": user_query},
            {"role": "assistant", "content": error_msg}
        ]
        return visible_history

def reset_conversation():
    """ëŒ€í™” ì´ˆê¸°í™”"""
    global conversation_history
    conversation_history = []
    return []

# Gradio ì¸í„°í˜ì´ìŠ¤
with gr.Blocks(title="nxj_llm RAG ì‹œìŠ¤í…œ") as demo:
    gr.Markdown("""
    # nxj_llm RAG ì‹œìŠ¤í…œ
    
    ì˜ë£Œì œí’ˆ ì¸í—ˆê°€ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. ì‹œìŠ¤í…œì´ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
    """)
    
    chatbot = gr.Chatbot(
        height=500,
        type="messages",
        label="ëŒ€í™”",
        elem_id="chatbot"
    )
    
    msg = gr.Textbox(
        label="ì§ˆë¬¸ ì…ë ¥",
        placeholder="ì˜ˆ: ì˜ë£Œìš© íœ ì²´ì–´ë€?",
        lines=2
    )
    
    with gr.Row():
        submit = gr.Button("ì „ì†¡", variant="primary")
        clear = gr.Button("ëŒ€í™” ì´ˆê¸°í™”")
    
    # ì˜ˆì œ ì§ˆë¬¸ë“¤
    gr.Examples(
        examples=[
            "ì˜ë£Œìš© íœ ì²´ì–´ë€?",
            "ì²´ì™¸ì§„ë‹¨ê¸°ê¸°ì˜ ë“±ê¸‰ ë¶„ë¥˜ ê¸°ì¤€ì€?",
            "ì˜ë£Œê¸°ê¸° ì„ìƒì‹œí—˜ ì ˆì°¨ëŠ”?",
            "GMP ì¸ì¦ ìš”êµ¬ì‚¬í•­ì€?"
        ],
        inputs=msg
    )
    
    # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
    msg.submit(respond_wrapper, [msg, chatbot], [chatbot]).then(
        lambda: "", None, [msg]
    )
    submit.click(respond_wrapper, [msg, chatbot], [chatbot]).then(
        lambda: "", None, [msg]
    )
    clear.click(reset_conversation, None, [chatbot])
    
    # ìƒíƒœ í‘œì‹œ
    gr.Markdown("""
    ---
    ğŸ’¡ **ì‚¬ìš© íŒ**:
    - êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - ë‹µë³€ì—ëŠ” ì¶œì²˜ ì •ë³´ê°€ í•¨ê»˜ ì œê³µë©ë‹ˆë‹¤.
    - ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼ìœ¼ë¡œ ìƒˆë¡œìš´ ëŒ€í™”ë¥¼ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """)

if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0", 
        server_port=7862, 
        share=False,
        show_error=True
    )
'''

# 4. registry.py
registry_content = '''"""
ë„êµ¬ ë ˆì§€ìŠ¤íŠ¸ë¦¬
RAG ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš©í•˜ëŠ” ëª¨ë“  ë„êµ¬ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
"""
from utils.tool_search import search_documents
from utils.tool_answer import final_answer
from utils.tool_diagnose import tool_diagnose

# í™œì„±í™”ëœ ë„êµ¬ ëª©ë¡
TOOLS = [
    search_documents,  # ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬
    final_answer,      # ìµœì¢… ë‹µë³€ ìƒì„± ë„êµ¬
    tool_diagnose      # ì‹œìŠ¤í…œ ì§„ë‹¨ ë„êµ¬
]

# ë„êµ¬ ì´ë¦„ìœ¼ë¡œ ë§¤í•‘ (ì„ íƒì )
TOOL_MAP = {tool.name: tool for tool in TOOLS}

# ë„êµ¬ ì •ë³´ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
def print_tool_info():
    """ë“±ë¡ëœ ë„êµ¬ ì •ë³´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    print("=== ë“±ë¡ëœ ë„êµ¬ ëª©ë¡ ===")
    for tool in TOOLS:
        print(f"- {tool.name}: {tool.description}")
    print(f"ì´ {len(TOOLS)}ê°œì˜ ë„êµ¬ê°€ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    print_tool_info()
'''

# 5. utils/tool_search.py
tool_search_content = '''import logging
import json
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from utils.embed_query import embed_query
from utils.load_all_embeddings import load_all_embeddings
from pathlib import Path
from collections import defaultdict
from langchain_core.tools import BaseTool

logger = logging.getLogger(__name__)

class SearchDocumentsTool(BaseTool):
    name: str = "search_documents"
    description: str = "ì¿¼ë¦¬ì— ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œ ì²­í¬ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."

    def _run(self, query: str) -> str:
        """
        ë¬¸ì„œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ JSON ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            
        Returns:
            JSON ë¬¸ìì—´ í˜•íƒœì˜ ê²€ìƒ‰ ê²°ê³¼
        """
        try:
            logger.info(f"[SearchDocuments] Query: {query}")
            
            # 1. ì„ë² ë”© ë°ì´í„° ë¡œë“œ
            docs_all, embs_all, srcs_all = load_all_embeddings()
            logger.info(f"[SearchDocuments] Loaded {len(embs_all)} chunks from {len(set(srcs_all))} files")
            
            if len(embs_all) == 0:
                result = {
                    "documents": [],
                    "sources": [],
                    "paths": [],
                    "message": "ê²€ìƒ‰í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
                }
                return json.dumps(result, ensure_ascii=False)
            
            # 2. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_vec = embed_query(query)
            if query_vec is None or not isinstance(query_vec, np.ndarray):
                result = {
                    "documents": [],
                    "sources": [],
                    "paths": [],
                    "error": "ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨"
                }
                return json.dumps(result, ensure_ascii=False)
            
            # 3. ìœ íš¨í•œ ì„ë² ë”©ë§Œ í•„í„°ë§
            filtered = [(d, e, s) for d, e, s in zip(docs_all, embs_all, srcs_all) if e is not None]
            if not filtered:
                result = {
                    "documents": [],
                    "sources": [],
                    "paths": [],
                    "message": "ìœ íš¨í•œ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤."
                }
                return json.dumps(result, ensure_ascii=False)
            
            docs, embs, srcs = zip(*filtered)
            
            # 4. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            embs_arr = np.vstack(embs)
            sims = cosine_similarity([query_vec], embs_arr)[0]
            
            logger.info(f"[SearchDocuments] Similarity - max: {sims.max():.4f}, mean: {sims.mean():.4f}")
            
            # 5. ìƒìœ„ ì²­í¬ ì„ íƒ
            top_k_chunks = min(100, len(sims))
            top_idx = sims.argsort()[-top_k_chunks:][::-1]
            
            docs = [docs[i] for i in top_idx]
            srcs = [srcs[i] for i in top_idx]
            sims = sims[top_idx]
            
            # 6. ë¬¸ì„œë³„ í‰ê·  ì ìˆ˜ë¡œ ìƒìœ„ íŒŒì¼ ì„ ì •
            file_scores = defaultdict(list)
            for sim, src in zip(sims, srcs):
                file_scores[src].append(sim)
            
            avg_scores = [(src, sum(scores)/len(scores)) for src, scores in file_scores.items()]
            top_files = [src for src, _ in sorted(avg_scores, key=lambda x: x[1], reverse=True)[:10]]
            
            # 7. ìµœì¢… ê²°ê³¼ ì„ íƒ (ìƒìœ„ 5ê°œ)
            final_results = []
            for src in top_files:
                file_chunks = [(sim, doc) for sim, doc, s in zip(sims, docs, srcs) if s == src]
                for sim, doc in sorted(file_chunks, key=lambda x: x[0], reverse=True):
                    final_results.append((sim, doc, src))
            
            # ìƒìœ„ 5ê°œë§Œ ì„ íƒ
            final_results = sorted(final_results, key=lambda x: x[0], reverse=True)[:5]
            
            # 8. ê²°ê³¼ í¬ë§·íŒ…
            result = {
                "documents": [doc for _, doc, _ in final_results],
                "sources": [Path(src).name for _, _, src in final_results],
                "paths": [str(Path(src)) for _, _, src in final_results],
                "scores": [float(sim) for sim, _, _ in final_results]  # ì ìˆ˜ë„ í¬í•¨
            }
            
            logger.info(f"[SearchDocuments] Found {len(result['documents'])} relevant documents")
            logger.info(f"[SearchDocuments] Sources: {result['sources']}")
            
            # JSON ë¬¸ìì—´ë¡œ ë°˜í™˜
            return json.dumps(result, ensure_ascii=False)
            
        except Exception as e:
            logger.error(f"[SearchDocuments] Error: {e}", exc_info=True)
            result = {
                "documents": [],
                "sources": [],
                "paths": [],
                "error": f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            }
            return json.dumps(result, ensure_ascii=False)

    async def _arun(self, query: str) -> str:
        return self._run(query)

# ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
search_documents = SearchDocumentsTool()
'''

# 6. utils/tool_answer.py
tool_answer_content = '''from langchain_core.tools import BaseTool
from typing import List
import json


class FinalAnswerTool(BaseTool):
    name: str = "final_answer"  # ì´ë¦„ í†µì¼
    description: str = "ìµœì¢… ë‹µë³€ê³¼ ì¶œì²˜ ë° ê²½ë¡œë¥¼ í¬í•¨í•œ ì‚¬ì „(dict)ì„ ìƒì„±í•©ë‹ˆë‹¤."

    def _run(self, answer: str, sources: List[str] = None, paths: List[str] = None) -> str:
        """
        ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            answer: ìµœì¢… ë‹µë³€ í…ìŠ¤íŠ¸
            sources: ì¶œì²˜ íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸ (ì˜µì…˜)
            paths: íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (ì˜µì…˜)
            
        Returns:
            JSON ë¬¸ìì—´ í˜•íƒœì˜ ë‹µë³€
        """
        # ê¸°ë³¸ê°’ ì²˜ë¦¬
        sources = sources or []
        paths = paths or []
        
        # ì¶œì²˜ê°€ ìˆëŠ” ê²½ìš° ë‹µë³€ì— ì¶”ê°€
        if sources:
            answer_with_sources = f"{answer}\\n\\nğŸ“š ì¶œì²˜: {', '.join(sources)}"
        else:
            answer_with_sources = answer
            
        output = {
            "answer": answer_with_sources,
            "sources": sources,
            "paths": paths
        }
        
        # JSON ë¬¸ìì—´ë¡œ ë°˜í™˜ (ToolNodeê°€ íŒŒì‹±í•  ìˆ˜ ìˆë„ë¡)
        return json.dumps(output, ensure_ascii=False)

    async def _arun(self, answer: str, sources: List[str] = None, paths: List[str] = None) -> str:
        return self._run(answer, sources, paths)

# ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
final_answer = FinalAnswerTool()
'''

# 7. utils/tool_diagnose.py
tool_diagnose_content = '''from langchain_core.tools import BaseTool
import json
from typing import Dict, Any
import traceback

class ToolDiagnoseTool(BaseTool):
    name: str = "tool_diagnose"
    description: str = "ì‹œìŠ¤í…œ ë¬¸ì œë¥¼ ì§„ë‹¨í•˜ê³  ì›ì¸ì„ ë¶„ì„í•©ë‹ˆë‹¤."

    def _run(self, last_input: str = "", context: Dict[str, Any] = None) -> str:
        """
        ì‹œìŠ¤í…œ ë¬¸ì œë¥¼ ì§„ë‹¨í•©ë‹ˆë‹¤.
        
        Args:
            last_input: ë§ˆì§€ë§‰ ì‚¬ìš©ì ì…ë ¥
            context: ì§„ë‹¨ì— í•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
            
        Returns:
            JSON ë¬¸ìì—´ í˜•íƒœì˜ ì§„ë‹¨ ê²°ê³¼
        """
        context = context or {}
        diagnosis = {
            "status": "diagnosed",
            "issues": [],
            "recommendations": []
        }
        
        # 1. ì…ë ¥ ê²€ì¦
        if not last_input:
            diagnosis["issues"].append("â— ì‚¬ìš©ì ì…ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.")
            diagnosis["recommendations"].append("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        # 2. ìƒíƒœ ê²€ì¦
        if "messages" in context:
            msg_count = len(context["messages"])
            if msg_count == 0:
                diagnosis["issues"].append("â— ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                diagnosis["recommendations"].append("ìƒˆë¡œìš´ ëŒ€í™”ë¥¼ ì‹œì‘í•´ì£¼ì„¸ìš”.")
            else:
                diagnosis["issues"].append(f"âœ“ ë©”ì‹œì§€ ìˆ˜: {msg_count}ê°œ")
        
        # 3. ì—ëŸ¬ ì •ë³´ í™•ì¸
        if "error" in context:
            error_msg = str(context["error"])
            diagnosis["issues"].append(f"â— ì˜¤ë¥˜ ë°œìƒ: {error_msg}")
            
            # ì¼ë°˜ì ì¸ ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„
            if "API" in error_msg or "api" in error_msg:
                diagnosis["recommendations"].append("API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            elif "embedding" in error_msg or "ì„ë² ë”©" in error_msg:
                diagnosis["recommendations"].append("ì„ë² ë”© ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            elif "connection" in error_msg or "ì—°ê²°" in error_msg:
                diagnosis["recommendations"].append("ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            else:
                diagnosis["recommendations"].append("ì‹œìŠ¤í…œ ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        # 4. ë„êµ¬ ìƒíƒœ í™•ì¸
        if "tools_status" in context:
            for tool_name, status in context["tools_status"].items():
                if status == "failed":
                    diagnosis["issues"].append(f"â— {tool_name} ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨")
                else:
                    diagnosis["issues"].append(f"âœ“ {tool_name} ë„êµ¬ ì •ìƒ")
        
        # 5. ì„ë² ë”© ë°ì´í„° í™•ì¸
        try:
            from utils.load_all_embeddings import load_all_embeddings
            docs, embs, srcs = load_all_embeddings()
            if len(docs) == 0:
                diagnosis["issues"].append("â— ì„ë² ë”© ë°ì´í„°ë² ì´ìŠ¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                diagnosis["recommendations"].append("ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•´ì£¼ì„¸ìš”.")
            else:
                diagnosis["issues"].append(f"âœ“ ì„ë² ë”© ë°ì´í„°: {len(docs)}ê°œ ë¬¸ì„œ, {len(set(srcs))}ê°œ íŒŒì¼")
        except Exception as e:
            diagnosis["issues"].append(f"â— ì„ë² ë”© ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            diagnosis["recommendations"].append("ì„ë² ë”© íŒŒì¼ ê²½ë¡œì™€ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        # 6. LLM ì—°ê²° í™•ì¸
        if "llm_error" in context:
            diagnosis["issues"].append("â— LLM ì—°ê²° ì˜¤ë¥˜")
            diagnosis["recommendations"].append("OpenAI API í‚¤ì™€ ë„¤íŠ¸ì›Œí¬ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        # 7. ìµœì¢… ì§„ë‹¨
        if not diagnosis["issues"]:
            diagnosis["issues"].append("âœ“ ì‹œìŠ¤í…œ ìƒíƒœê°€ ì •ìƒì…ë‹ˆë‹¤.")
        
        if not diagnosis["recommendations"]:
            diagnosis["recommendations"].append("ì‹œìŠ¤í…œì´ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤.")
        
        # ì§„ë‹¨ ê²°ê³¼ í¬ë§·íŒ…
        result = {
            "diagnosis": "\\n".join(diagnosis["issues"]),
            "recommendations": "\\n".join(diagnosis["recommendations"]),
            "details": diagnosis
        }
        
        return json.dumps(result, ensure_ascii=False, indent=2)

    async def _arun(self, last_input: str = "", context: Dict[str, Any] = None) -> str:
        return self._run(last_input, context)

# ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
tool_diagnose = ToolDiagnoseTool()
'''

# 8. test_rag.py
test_rag_content = '''#!/usr/bin/env python3
"""
RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ê° ì»´í¬ë„ŒíŠ¸ë¥¼ ê°œë³„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ì—¬ ë¬¸ì œì ì„ ì°¾ìŠµë‹ˆë‹¤.
"""
import os
import sys
import logging
from langchain_core.messages import HumanMessage

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.DEBUG,
    format="[%(asctime)s] %(name)s - %(levelname)s: %(message)s"
)

# API í‚¤ ì„¤ì •
os.environ["OPENAI_API_KEY"] = "YOUR_API_KEY_HERE"  # ì‹¤ì œ í‚¤ë¡œ êµì²´ í•„ìš”

def test_tools():
    """ë„êµ¬ë“¤ì´ ì œëŒ€ë¡œ ì‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸"""
    print("\\n=== ë„êµ¬ í…ŒìŠ¤íŠ¸ ===")
    
    # 1. search_documents í…ŒìŠ¤íŠ¸
    try:
        from utils.tool_search import search_documents
        print(f"âœ“ search_documents ë¡œë“œ ì„±ê³µ (name: {search_documents.name})")
        
        # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        result = search_documents._run("ì˜ë£Œìš© íœ ì²´ì–´")
        print(f"  ê²€ìƒ‰ ê²°ê³¼: {result[:100]}...")
    except Exception as e:
        print(f"âœ— search_documents ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
    
    # 2. final_answer í…ŒìŠ¤íŠ¸
    try:
        from utils.tool_answer import final_answer
        print(f"\\nâœ“ final_answer ë¡œë“œ ì„±ê³µ (name: {final_answer.name})")
        
        # ë‹µë³€ ìƒì„± í…ŒìŠ¤íŠ¸
        result = final_answer._run(
            answer="í…ŒìŠ¤íŠ¸ ë‹µë³€ì…ë‹ˆë‹¤.",
            sources=["test.pdf"],
            paths=["/path/to/test.pdf"]
        )
        print(f"  ë‹µë³€ ê²°ê³¼: {result}")
    except Exception as e:
        print(f"âœ— final_answer ì˜¤ë¥˜: {e}")
    
    # 3. tool_diagnose í…ŒìŠ¤íŠ¸
    try:
        from utils.tool_diagnose import tool_diagnose
        print(f"\\nâœ“ tool_diagnose ë¡œë“œ ì„±ê³µ (name: {tool_diagnose.name})")
    except Exception as e:
        print(f"âœ— tool_diagnose ì˜¤ë¥˜: {e}")

def test_llm():
    """LLM ì—°ê²° í…ŒìŠ¤íŠ¸"""
    print("\\n\\n=== LLM í…ŒìŠ¤íŠ¸ ===")
    
    try:
        from langchain_openai import ChatOpenAI
        llm = ChatOpenAI(model="gpt-4-turbo", temperature=0.2)
        
        response = llm.invoke([HumanMessage(content="Hello, are you working?")])
        print(f"âœ“ LLM ì‘ë‹µ: {response.content}")
    except Exception as e:
        print(f"âœ— LLM ì˜¤ë¥˜: {e}")
        print("  â†’ API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”")

def test_graph():
    """Graph ì „ì²´ í…ŒìŠ¤íŠ¸"""
    print("\\n\\n=== Graph í…ŒìŠ¤íŠ¸ ===")
    
    try:
        from graph_builder import respond
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
        state = {
            "messages": [HumanMessage(content="ì˜ë£Œìš© íœ ì²´ì–´ë€ ë¬´ì—‡ì¸ê°€ìš”?")],
            "query": "ì˜ë£Œìš© íœ ì²´ì–´ë€ ë¬´ì—‡ì¸ê°€ìš”?"
        }
        
        print("ì…ë ¥:", state)
        result = respond(state)
        print("\\nì¶œë ¥:")
        print(f"  - answer: {result.get('answer', 'N/A')[:100]}...")
        print(f"  - sources: {result.get('sources', [])}")
        print(f"  - output: {result.get('output', 'N/A')[:100]}...")
        
    except Exception as e:
        print(f"âœ— Graph ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

def test_embeddings():
    """ì„ë² ë”© ë°ì´í„° í™•ì¸"""
    print("\\n\\n=== ì„ë² ë”© ë°ì´í„° í…ŒìŠ¤íŠ¸ ===")
    
    try:
        from utils.load_all_embeddings import load_all_embeddings
        docs, embs, srcs = load_all_embeddings()
        
        print(f"âœ“ ì´ ë¬¸ì„œ ìˆ˜: {len(docs)}")
        print(f"âœ“ ì´ íŒŒì¼ ìˆ˜: {len(set(srcs))}")
        
        if docs:
            print(f"âœ“ ì²« ë²ˆì§¸ ë¬¸ì„œ ìƒ˜í”Œ: {docs[0][:100]}...")
            print(f"âœ“ ì²« ë²ˆì§¸ ì†ŒìŠ¤: {srcs[0]}")
    except Exception as e:
        print(f"âœ— ì„ë² ë”© ë¡œë“œ ì˜¤ë¥˜: {e}")

def main():
    """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("RAG ì‹œìŠ¤í…œ ì§„ë‹¨ ì‹œì‘...")
    
    # 1. ì„ë² ë”© ë°ì´í„° í™•ì¸
    test_embeddings()
    
    # 2. ë„êµ¬ í…ŒìŠ¤íŠ¸
    test_tools()
    
    # 3. LLM í…ŒìŠ¤íŠ¸
    test_llm()
    
    # 4. ì „ì²´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
    test_graph()
    
    print("\\n\\n=== í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")

if __name__ == "__main__":
    main()
'''

# 9. utils/embed_query.py (ìƒ˜í”Œ)
embed_query_content = '''import numpy as np
from langchain_openai import OpenAIEmbeddings

embeddings_model = OpenAIEmbeddings()

def embed_query(query: str) -> np.ndarray:
    """ì¿¼ë¦¬ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
    try:
        embedding = embeddings_model.embed_query(query)
        return np.array(embedding)
    except Exception as e:
        print(f"ì„ë² ë”© ì˜¤ë¥˜: {e}")
        return None
'''

# 10. utils/load_all_embeddings.py (ìƒ˜í”Œ)
load_all_embeddings_content = '''import pickle
import numpy as np
from pathlib import Path

def load_all_embeddings():
    """ì €ì¥ëœ ì„ë² ë”© ë°ì´í„° ë¡œë“œ"""
    # ì‹¤ì œ êµ¬í˜„ì— ë§ê²Œ ìˆ˜ì • í•„ìš”
    embeddings_dir = Path("embeddings")
    
    docs_all = []
    embs_all = []
    srcs_all = []
    
    # ì˜ˆì‹œ: pickle íŒŒì¼ì—ì„œ ë¡œë“œ
    if embeddings_dir.exists():
        for emb_file in embeddings_dir.glob("*.pkl"):
            try:
                with open(emb_file, "rb") as f:
                    data = pickle.load(f)
                    docs_all.extend(data.get("documents", []))
                    embs_all.extend(data.get("embeddings", []))
                    srcs_all.extend(data.get("sources", []))
            except Exception as e:
                print(f"Error loading {emb_file}: {e}")
    
    # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°ì´í„° (ì‹¤ì œ ë°ì´í„°ê°€ ì—†ì„ ê²½ìš°)
    if not docs_all:
        print("âš ï¸  ì‹¤ì œ ì„ë² ë”© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°ì´í„° ì‚¬ìš©")
        docs_all = ["ì˜ë£Œìš© íœ ì²´ì–´ëŠ” ë³´í–‰ì´ ë¶ˆí¸í•œ í™˜ìë¥¼ ìœ„í•œ ì˜ë£Œê¸°ê¸°ì…ë‹ˆë‹¤."]
        embs_all = [np.random.rand(1536)]  # OpenAI ì„ë² ë”© ì°¨ì›
        srcs_all = ["dummy_document.pdf"]
    
    return docs_all, embs_all, srcs_all
'''

# 11. requirements.txt
requirements_content = '''langchain>=0.1.0
langchain-openai>=0.0.5
langgraph>=0.0.37
gradio>=4.0.0
numpy>=1.24.0
scikit-learn>=1.3.0
python-dotenv>=1.0.0
'''

# 12. .env (ìƒ˜í”Œ)
env_content = '''# OpenAI API Key
OPENAI_API_KEY=your-openai-api-key-here

# ê¸°íƒ€ í™˜ê²½ ë³€ìˆ˜
LOG_LEVEL=INFO
'''

# 13. README.md
readme_content = '''# RAG ì‹œìŠ¤í…œ (nxj_llm)

ëŒ€í•œë¯¼êµ­ ì‹í’ˆì˜ì•½í’ˆì•ˆì „ì²˜ ì˜ë£Œì œí’ˆ ì¸í—ˆê°€ ì§€ì›ì„ ìœ„í•œ RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## ì£¼ìš” ê¸°ëŠ¥

- ë¬¸ì„œ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±
- LangGraph ê¸°ë°˜ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ
- Function Callingì„ í†µí•œ ë„êµ¬ ì‹¤í–‰
- Gradio ì›¹ ì¸í„°í˜ì´ìŠ¤

## ì„¤ì¹˜ ë°©ë²•

1. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜:
```bash
pip install -r requirements.txt
```

2. OpenAI API í‚¤ ì„¤ì •:
```bash
cp .env.example .env
# .env íŒŒì¼ì„ í¸ì§‘í•˜ì—¬ API í‚¤ ì…ë ¥
```

3. ì„ë² ë”© ë°ì´í„° ì¤€ë¹„:
- `embeddings/` ë””ë ‰í† ë¦¬ì— ì„ë² ë”© íŒŒì¼ ë°°ì¹˜
- ë˜ëŠ” `utils/load_all_embeddings.py` ìˆ˜ì •

## ì‹¤í–‰ ë°©ë²•

### ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
```bash
python test_rag.py
```

### ì›¹ ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰
```bash
python run_gradio.py
```

ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:7862 ì ‘ì†

## íŒŒì¼ êµ¬ì¡°

- `graph_builder.py`: ë©”ì¸ ê·¸ë˜í”„ ë¡œì§
- `state_types.py`: State íƒ€ì… ì •ì˜
- `run_gradio.py`: Gradio UI
- `registry.py`: ë„êµ¬ ë ˆì§€ìŠ¤íŠ¸ë¦¬
- `utils/`: ë„êµ¬ ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
  - `tool_search.py`: ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬
  - `tool_answer.py`: ë‹µë³€ ìƒì„± ë„êµ¬
  - `tool_diagnose.py`: ì‹œìŠ¤í…œ ì§„ë‹¨ ë„êµ¬

## ë¬¸ì œ í•´ê²°

ì„ë² ë”© ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°, `utils/load_all_embeddings.py`ì˜ ë”ë¯¸ ë°ì´í„°ê°€ ì‚¬ìš©ë©ë‹ˆë‹¤.
ì‹¤ì œ ì‚¬ìš©ì„ ìœ„í•´ì„œëŠ” ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•´ì•¼ í•©ë‹ˆë‹¤.
'''

# íŒŒì¼ ìƒì„±
create_file("graph_builder.py", graph_builder_content)
create_file("state_types.py", state_types_content)
create_file("run_gradio.py", run_gradio_content)
create_file("registry.py", registry_content)
create_file("utils/tool_search.py", tool_search_content)
create_file("utils/tool_answer.py", tool_answer_content)
create_file("utils/tool_diagnose.py", tool_diagnose_content)
create_file("test_rag.py", test_rag_content)
create_file("utils/embed_query.py", embed_query_content)
create_file("utils/load_all_embeddings.py", load_all_embeddings_content)
create_file("requirements.txt", requirements_content)
create_file(".env.example", env_content)
create_file("README.md", readme_content)

# utils/__init__.py ìƒì„± (íŒ¨í‚¤ì§€ ì¸ì‹ìš©)
create_file("utils/__init__.py", "# Utils package")

print("\nâœ… ëª¨ë“  íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
print("\në‹¤ìŒ ë‹¨ê³„:")
print("1. OpenAI API í‚¤ ì„¤ì •: .env.exampleì„ .envë¡œ ë³µì‚¬í•˜ê³  í‚¤ ì…ë ¥")
print("2. íŒ¨í‚¤ì§€ ì„¤ì¹˜: pip install -r requirements.txt")
print("3. ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸: python test_rag.py")
print("4. UI ì‹¤í–‰: python run_gradio.py")