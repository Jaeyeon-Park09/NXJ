import gradio as gr
from graph_builder import respond as graph_respond
from langchain_core.messages import HumanMessage, AIMessage
import json
import logging
import importlib.util
import os

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="[%(asctime)s] %(levelname)s: %(message)s",
)

# ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬
conversation_history: list[dict] = []

def extract_answer(response: dict) -> str:
    """
    ì‘ë‹µì—ì„œ ë‹µë³€ ì¶”ì¶œ
    """
    # 1. answer í•„ë“œ í™•ì¸
    if response.get("answer"):
        answer = response["answer"]
        if isinstance(answer, list):
            return "\n".join(str(a) for a in answer)
        return answer
    
    # 2. output í•„ë“œ í™•ì¸
    if response.get("output"):
        output = response["output"]
        if isinstance(output, list):
            return "\n".join(str(a) for a in output)
        return output
    
    # 3. messagesì—ì„œ ë§ˆì§€ë§‰ AI ë©”ì‹œì§€ ì¶”ì¶œ
    messages = response.get("messages", [])
    for msg in reversed(messages):
        if isinstance(msg, AIMessage) and msg.content:
            content = msg.content
            if isinstance(content, list):
                return "\n".join(str(a) for a in content)
            # JSON í˜•íƒœì˜ ì‘ë‹µì¸ì§€ í™•ì¸
            if isinstance(content, str) and content.strip().startswith("{"):
                try:
                    data = json.loads(content)
                    if "answer" in data:
                        answer = data["answer"]
                        if isinstance(answer, list):
                            return "\n".join(str(a) for a in answer)
                        return answer
                except:
                    pass
            return content
    
    return "âŒ ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

def respond_wrapper(message: str, chat_history: list | None = None) -> list:
    """Gradio ì¸í„°í˜ì´ìŠ¤ë¥¼ ìœ„í•œ ì‘ë‹µ í•¨ìˆ˜"""
    logging.info(f"USER: {message}")
    
    if not message.strip():
        return chat_history or []
    
    # íˆìŠ¤í† ë¦¬ ê´€ë¦¬
    history = chat_history or []
    user_query = message.strip()
    
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
    conversation_history.append({"role": "user", "content": user_query})
    
    # 'ë³´ê³ ì„œ' ë˜ëŠ” 'ë¦¬í¬íŠ¸' í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš° report_generation toolì„ ìš°ì„  í˜¸ì¶œ
    if any(keyword in user_query for keyword in ["ë³´ê³ ì„œ", "ë¦¬í¬íŠ¸"]):
        # report_generation tool ë™ì  import
        spec = importlib.util.spec_from_file_location("tool_reportgen", os.path.join(os.path.dirname(__file__), "utils", "tool_reportgen.py"))
        if spec is not None and spec.loader is not None:
            tool_reportgen = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(tool_reportgen)
            report_generation = tool_reportgen.report_generation
            report_prompt = report_generation._run(topic=user_query)
            answer = report_prompt
            conversation_history.append({"role": "assistant", "content": answer})
            visible_history = [
                {"role": "user", "content": user_query},
                {"role": "assistant", "content": answer}
            ]
            logging.info(f"ASSISTANT: {answer[:100]}...")
            return visible_history
        else:
            error_msg = "âŒ report_generation tool import ì˜¤ë¥˜"
            conversation_history.append({"role": "assistant", "content": error_msg})
            visible_history = [
                {"role": "user", "content": user_query},
                {"role": "assistant", "content": error_msg}
            ]
            return visible_history
    
    # LangChain ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    messages = []
    for msg in conversation_history:
        if msg["role"] == "user":
            messages.append(HumanMessage(content=msg["content"]))
        elif msg["role"] == "assistant":
            messages.append(AIMessage(content=msg["content"]))
    
    # Graph ìƒíƒœ ìƒì„±
    state = {
        "messages": messages,
        "query": user_query,
        "last_user": user_query
    }
    
    try:
        # Graph ì‹¤í–‰
        response = graph_respond(state)
        logging.debug(f"Graph response: {response}")
        
        # ë‹µë³€ ì¶”ì¶œ
        answer = extract_answer(response)
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        conversation_history.append({"role": "assistant", "content": answer})
        
        # UIì— í‘œì‹œí•  íˆìŠ¤í† ë¦¬ (ìµœê·¼ ëŒ€í™”ë§Œ)
        visible_history = [
            {"role": "user", "content": user_query},
            {"role": "assistant", "content": answer}
        ]
        
        logging.info(f"ASSISTANT: {answer[:100]}...")
        return visible_history
        
    except Exception as e:
        logging.exception("Error in graph_respond")
        error_msg = f"âš ï¸ ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        
        visible_history = [
            {"role": "user", "content": user_query},
            {"role": "assistant", "content": error_msg}
        ]
        return visible_history

def reset_conversation():
    """ëŒ€í™” ì´ˆê¸°í™”"""
    global conversation_history
    conversation_history = []
    return []

# Gradio ì¸í„°í˜ì´ìŠ¤
with gr.Blocks(title="nxj_llm RAG test") as demo:
    gr.Markdown("""
    # RAG test
    
    ì˜ë£Œì œí’ˆ ì¸í—ˆê°€ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. ì‹œìŠ¤í…œì´ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
    """)
    
    chatbot = gr.Chatbot(
        height=500,
        type="messages",
        label="ëŒ€í™”",
        elem_id="chatbot"
    )
    
    msg = gr.Textbox(
        label="ì§ˆë¬¸ ì…ë ¥",
        placeholder="ì˜ˆ: ì˜ë£Œìš© íœ ì²´ì–´ë€?",
        lines=2
    )
    
    with gr.Row():
        submit = gr.Button("ì „ì†¡", variant="primary")
        clear = gr.Button("ëŒ€í™” ì´ˆê¸°í™”")
    
    # ì˜ˆì œ ì§ˆë¬¸ë“¤
    gr.Examples(
        examples=[
            "ì˜ë£Œìš© íœ ì²´ì–´ë€?",
            "ì²´ì™¸ì§„ë‹¨ê¸°ê¸°ì˜ ë“±ê¸‰ ë¶„ë¥˜ ê¸°ì¤€ì€?",
            "ì˜ë£Œê¸°ê¸° ì„ìƒì‹œí—˜ ì ˆì°¨ëŠ”?",
            "GMP ì¸ì¦ ìš”êµ¬ì‚¬í•­ì€?"
        ],
        inputs=msg
    )
    
    # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
    msg.submit(respond_wrapper, [msg, chatbot], [chatbot]).then(
        lambda: "", None, [msg]
    )
    submit.click(respond_wrapper, [msg, chatbot], [chatbot]).then(
        lambda: "", None, [msg]
    )
    clear.click(reset_conversation, None, [chatbot])
    
    # ìƒíƒœ í‘œì‹œ
    gr.Markdown("""
    ---
    ğŸ’¡ **ì‚¬ìš© íŒ**:
    - êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - ë‹µë³€ì—ëŠ” ì¶œì²˜ ì •ë³´ê°€ í•¨ê»˜ ì œê³µë©ë‹ˆë‹¤.
    - ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼ìœ¼ë¡œ ìƒˆë¡œìš´ ëŒ€í™”ë¥¼ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """)

if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0", 
        server_port=7862, 
        share=True,
        show_error=True
    )
